# experiments/run_single_distribution.py

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
import yaml
import argparse
import logging
import time
from datetime import datetime
from src.data.mnist_data import (
    load_mnist_data,
    create_iid_partition,
    create_client_loaders,
    get_test_loader
)
from src.models.cnn_model import MNIST_CNN
from src.client_selection.uniform_selector import UniformSelector
from src.client_selection.binomial_selector import BinomialSelector
from src.client_selection.poisson_selector import PoissonSelector
from src.client_selection.normal_selector import NormalSelector
from src.client_selection.exponential_selector import ExponentialSelector
from src.algorithms.fedavg import FederatedAveraging


def setup_logging(distribution_name, log_dir='../results/logs'):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = os.path.join(log_dir, f"{distribution_name}_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"="*80)
    logger.info(f"å¼€å§‹ {distribution_name.upper()} åˆ†å¸ƒå®éªŒ")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_filename}")
    logger.info(f"="*80)
    
    return logger, log_filename


def get_distribution_config(distribution_name):
    """è·å–å„åˆ†å¸ƒçš„é»˜è®¤é…ç½®"""
    configs = {
        'uniform': {},
        'binomial': {'alpha': 2, 'beta': 5, 'static': True},
        'poisson': {'lambda': 5, 'static': True},
        'normal': {'sigma': 1.0, 'weight_method': 'spatial'},
        'exponential': {'lambda': 1.0, 'exp_mode': 'decay'}
    }
    return configs.get(distribution_name, {})


def run_single_distribution(distribution_name, custom_config=None):
    """è¿è¡Œå•ä¸ªåˆ†å¸ƒçš„å®éªŒ"""
    
    # è®¾ç½®æ—¥å¿—è®°å½•
    logger, log_filename = setup_logging(distribution_name)
    start_time = time.time()
    
    logger.info(f"ğŸ¯ å•åˆ†å¸ƒå®éªŒï¼š{distribution_name.upper()}")
    
    # åŠ è½½åŸºç¡€é…ç½®
    config_path = os.path.join(os.path.dirname(__file__), '..', 'configs', 'base_config.yaml')
    with open(config_path, 'r') as f:
        base_config = yaml.safe_load(f)
    
    # è·å–åˆ†å¸ƒé…ç½®
    if custom_config:
        selector_config = custom_config
    else:
        selector_config = get_distribution_config(distribution_name)
    
    logger.info(f"ğŸ“Š åˆ†å¸ƒç±»å‹: {distribution_name}")
    logger.info(f"âš™ï¸  åˆ†å¸ƒå‚æ•°: {selector_config}")
    logger.info(f"ğŸ”§ åŸºç¡€é…ç½®: å®¢æˆ·ç«¯æ•°={base_config['data']['num_clients']}, æ¯è½®é€‰æ‹©={base_config['federated']['clients_per_round']}, è½®æ•°={base_config['training']['num_rounds']}")
    logger.info("-"*80)
    
    # è®¾ç½®éšæœºç§å­
    seed = base_config['experiment']['seed']
    torch.manual_seed(seed)
    np.random.seed(seed)
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    
    # åŠ è½½æ•°æ®
    logger.info("1ï¸âƒ£ åŠ è½½MNISTæ•°æ®...")
    data_dir = os.path.join(os.path.dirname(__file__), '..', base_config['data']['data_dir'])
    train_dataset, test_dataset = load_mnist_data(data_dir)
    test_loader = get_test_loader(test_dataset)
    logger.info(f"   è®­ç»ƒé›†å¤§å°: {len(train_dataset):,}, æµ‹è¯•é›†å¤§å°: {len(test_dataset):,}")
    
    # åˆ›å»ºIIDåˆ’åˆ†
    logger.info("2ï¸âƒ£ åˆ›å»ºIIDæ•°æ®åˆ’åˆ†...")
    client_indices = create_iid_partition(
        train_dataset,
        num_clients=base_config['data']['num_clients'],
        seed=seed
    )
    logger.info(f"   åˆ’åˆ†ä¸º {len(client_indices)} ä¸ªå®¢æˆ·ç«¯ï¼Œæ¯å®¢æˆ·ç«¯çº¦ {len(client_indices[0])} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºå®¢æˆ·ç«¯åŠ è½½å™¨
    client_loaders = create_client_loaders(
        train_dataset,
        client_indices,
        batch_size=base_config['federated']['batch_size']
    )
    logger.info(f"   å®¢æˆ·ç«¯åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹å¤§å°: {base_config['federated']['batch_size']}")
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("3ï¸âƒ£ åˆ›å»ºCNNæ¨¡å‹...")
    model = MNIST_CNN()
    param_count = model.count_parameters()
    logger.info(f"   æ¨¡å‹å‚æ•°é‡: {param_count:,}")
    
    # åˆ›å»ºé€‰æ‹©å™¨
    logger.info("4ï¸âƒ£ åˆ›å»ºå®¢æˆ·ç«¯é€‰æ‹©å™¨...")
    selector_classes = {
        'uniform': UniformSelector,
        'binomial': BinomialSelector,
        'poisson': PoissonSelector,
        'normal': NormalSelector,
        'exponential': ExponentialSelector
    }
    
    if distribution_name not in selector_classes:
        raise ValueError(f"æœªçŸ¥çš„åˆ†å¸ƒç±»å‹: {distribution_name}ã€‚æ”¯æŒçš„ç±»å‹: {list(selector_classes.keys())}")
    
    selector = selector_classes[distribution_name](
        num_clients=base_config['data']['num_clients'],
        config=selector_config
    )
    
    logger.info(f"   é€‰æ‹©å™¨ç±»å‹: {selector.get_name()}")
    logger.info(f"   é€‰æ‹©å™¨é…ç½®: {selector_config}")
    
    # åˆ›å»ºFedAvg
    logger.info("5ï¸âƒ£ åˆå§‹åŒ–FederatedAveraging...")
    fed_config = {
        'clients_per_round': base_config['federated']['clients_per_round'],
        'local_epochs': base_config['federated']['local_epochs'],
        'learning_rate': base_config['training']['learning_rate'],
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    logger.info(f"   è®­ç»ƒè®¾å¤‡: {fed_config['device']}")
    logger.info(f"   æ¯è½®é€‰æ‹©å®¢æˆ·ç«¯æ•°: {fed_config['clients_per_round']}")
    logger.info(f"   æœ¬åœ°è®­ç»ƒè½®æ•°: {fed_config['local_epochs']}")
    logger.info(f"   å­¦ä¹ ç‡: {fed_config['learning_rate']}")
    
    fed_alg = FederatedAveraging(
        model=model,
        client_loaders=client_loaders,
        test_loader=test_loader,
        client_selector=selector,
        config=fed_config
    )
    
    # è®­ç»ƒ
    logger.info("6ï¸âƒ£ å¼€å§‹è®­ç»ƒ...")
    train_start_time = time.time()
    history, selection_stats = fed_alg.train(
        num_rounds=base_config['training']['num_rounds'],
        eval_every=base_config['training']['eval_every'],
        logger=logger
    )
    train_end_time = time.time()
    training_duration = train_end_time - train_start_time
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_duration:.1f} ç§’ ({training_duration/60:.1f} åˆ†é’Ÿ)")
    
    # ä¿å­˜ç»“æœ
    logger.info("7ï¸âƒ£ ä¿å­˜ç»“æœ...")
    
    # è®¡ç®—æ€»è€—æ—¶
    total_duration = time.time() - start_time
    
    results = {
        'distribution': distribution_name,
        'config': selector_config,
        'base_config': base_config,
        'history': history,
        'selection_stats': selection_stats,
        'log_filename': log_filename,
        'training_duration': training_duration,
        'total_duration': total_duration,
        'timestamp': datetime.now().isoformat()
    }
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = os.path.join(os.path.dirname(__file__), '..', 'results')
    models_dir = os.path.join(results_dir, 'models')
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(models_dir, exist_ok=True)
    
    # ä¿å­˜æ–‡ä»¶
    result_file = os.path.join(results_dir, f'{distribution_name}_results.pt')
    model_file = os.path.join(models_dir, f'{distribution_name}_model.pt')
    
    torch.save(results, result_file)
    torch.save(model.state_dict(), model_file)
    
    logger.info(f"   ç»“æœæ–‡ä»¶: {result_file}")
    logger.info(f"   æ¨¡å‹æ–‡ä»¶: {model_file}")
    
    # è®°å½•è¯¦ç»†ç»“æœæ‘˜è¦
    logger.info("="*80)
    logger.info("ğŸ“Š å®éªŒç»“æœæ‘˜è¦")
    logger.info("="*80)
    logger.info(f"ğŸ¯ åˆ†å¸ƒç±»å‹: {distribution_name}")
    logger.info(f"ğŸ“ˆ æœ€ç»ˆå‡†ç¡®ç‡: {history['test_acc'][-1]:.4f}")
    logger.info(f"ğŸ“‰ æœ€ç»ˆæŸå¤±: {history['test_loss'][-1]:.4f}")
    
    # æ”¶æ•›åˆ†æ
    if len(history['test_acc']) > 1:
        initial_acc = history['test_acc'][0] if len(history['test_acc']) > 0 else 0
        final_acc = history['test_acc'][-1]
        improvement = final_acc - initial_acc
        logger.info(f"ğŸš€ å‡†ç¡®ç‡æå‡: {improvement:.4f} ({initial_acc:.4f} â†’ {final_acc:.4f})")
    
    # å®¢æˆ·ç«¯é€‰æ‹©ç»Ÿè®¡
    logger.info(f"ğŸ‘¥ å®¢æˆ·ç«¯é€‰æ‹©ç»Ÿè®¡:")
    logger.info(f"   - å¹³å‡é€‰æ‹©æ¬¡æ•°: {selection_stats['mean']:.1f}")
    logger.info(f"   - é€‰æ‹©æ ‡å‡†å·®: {selection_stats['std']:.1f}")
    logger.info(f"   - æœ€å¤§é€‰æ‹©æ¬¡æ•°: {selection_stats['max']}")
    logger.info(f"   - æœ€å°é€‰æ‹©æ¬¡æ•°: {selection_stats['min']}")
    fairness_ratio = selection_stats['max'] / max(selection_stats['min'], 1)
    logger.info(f"   - å…¬å¹³æ€§æ¯”ç‡ (è¶Šæ¥è¿‘1è¶Šå…¬å¹³): {fairness_ratio:.2f}")
    
    # æ—¶é—´ç»Ÿè®¡
    logger.info(f"â±ï¸ æ—¶é—´ç»Ÿè®¡:")
    logger.info(f"   - è®­ç»ƒè€—æ—¶: {training_duration:.1f} ç§’ ({training_duration/60:.1f} åˆ†é’Ÿ)")
    logger.info(f"   - æ€»è€—æ—¶: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)")
    logger.info(f"   - å¹³å‡æ¯è½®è€—æ—¶: {training_duration/base_config['training']['num_rounds']:.2f} ç§’")
    
    logger.info("="*80)
    logger.info(f"ğŸ‰ {distribution_name.upper()} åˆ†å¸ƒå®éªŒæˆåŠŸå®Œæˆï¼")
    logger.info(f"ğŸ“ å®Œæ•´æ—¥å¿—å·²ä¿å­˜è‡³: {log_filename}")
    logger.info("="*80)
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œå•ä¸ªåˆ†å¸ƒçš„è”é‚¦å­¦ä¹ å®éªŒ')
    parser.add_argument('distribution', 
                       choices=['uniform', 'binomial', 'poisson', 'normal', 'exponential'],
                       help='è¦è¿è¡Œçš„åˆ†å¸ƒç±»å‹')
    parser.add_argument('--rounds', type=int, default=None, 
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼)')
    parser.add_argument('--clients', type=int, default=None,
                       help='æ¯è½®é€‰æ‹©çš„å®¢æˆ·ç«¯æ•° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼)')
    
    args = parser.parse_args()
    
    # è‡ªå®šä¹‰é…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    custom_config = get_distribution_config(args.distribution)
    
    try:
        results = run_single_distribution(args.distribution, custom_config)
        print("\nğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
