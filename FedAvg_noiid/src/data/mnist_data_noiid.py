#!/usr/bin/env python3
"""
Non-IID MNISTæ•°æ®åŠ è½½å™¨
æ”¹è¿›ç‰ˆï¼šåˆ›å»ºçœŸæ­£çš„Non-IIDæ•°æ®åˆ†å¸ƒä»¥å±•ç¤ºä¸åŒå®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥çš„å·®å¼‚
"""

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import numpy as np


class NonIIDMNISTData:
    """Non-IID MNISTæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir='./data', num_clients=100, alpha=0.5, min_samples=50):
        """
        åˆå§‹åŒ–Non-IID MNISTæ•°æ®
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            num_clients: å®¢æˆ·ç«¯æ•°é‡
            alpha: Dirichletåˆ†å¸ƒå‚æ•°ï¼Œè¶Šå°è¶ŠNon-IID
            min_samples: æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ€å°æ ·æœ¬æ•°
        """
        self.data_dir = data_dir
        self.num_clients = num_clients
        self.alpha = alpha
        self.min_samples = min_samples
        
        # æ•°æ®å˜æ¢
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # åŠ è½½æ•°æ®
        self._load_data()
        self._create_noiid_partition()
    
    def _load_data(self):
        """åŠ è½½MNISTæ•°æ®é›†"""
        print("ğŸ“¥ åŠ è½½MNISTæ•°æ®é›†...")
        
        self.train_dataset = datasets.MNIST(
            self.data_dir, 
            train=True, 
            download=True, 
            transform=self.transform
        )
        
        self.test_dataset = datasets.MNIST(
            self.data_dir, 
            train=False, 
            download=True, 
            transform=self.transform
        )
        
        # æå–æ ‡ç­¾ç”¨äºåˆ†åŒº
        self.train_labels = np.array([self.train_dataset[i][1] for i in range(len(self.train_dataset))])
        
        print(f"   è®­ç»ƒé›†å¤§å°: {len(self.train_dataset):,}")
        print(f"   æµ‹è¯•é›†å¤§å°: {len(self.test_dataset):,}")
    
    def _create_noiid_partition(self):
        """åˆ›å»ºNon-IIDæ•°æ®åˆ†åŒº"""
        print(f"ğŸ”„ åˆ›å»ºNon-IIDæ•°æ®åˆ†åŒº (alpha={self.alpha})...")
        
        # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®ç´¢å¼•
        class_indices = {}
        for i, label in enumerate(self.train_labels):
            if label not in class_indices:
                class_indices[label] = []
            class_indices[label].append(i)
        
        # æ‰“ä¹±æ¯ä¸ªç±»åˆ«çš„ç´¢å¼•
        for label in class_indices:
            np.random.shuffle(class_indices[label])
        
        # ä½¿ç”¨Dirichletåˆ†å¸ƒç”Ÿæˆæ¯ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«åˆ†å¸ƒ
        num_classes = len(class_indices)
        client_class_distributions = np.random.dirichlet([self.alpha] * num_classes, self.num_clients)
        
        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯åº”è¯¥è·å¾—çš„æ ·æœ¬æ•°
        total_samples = len(self.train_dataset)
        base_samples_per_client = max(self.min_samples, total_samples // self.num_clients)
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…æ•°æ®
        self.client_indices = {}
        class_counters = {label: 0 for label in class_indices}
        
        for client_id in range(self.num_clients):
            client_indices = []
            client_samples = base_samples_per_client
            
            # æ ¹æ®Dirichletåˆ†å¸ƒä¸ºè¯¥å®¢æˆ·ç«¯åˆ†é…å„ç±»åˆ«çš„æ ·æœ¬æ•°
            client_class_counts = (client_class_distributions[client_id] * client_samples).astype(int)
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
            if client_class_counts.sum() == 0:
                client_class_counts[np.argmax(client_class_distributions[client_id])] = self.min_samples
            
            # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…æ ·æœ¬
            for class_label, count in enumerate(client_class_counts):
                if count > 0 and class_counters[class_label] < len(class_indices[class_label]):
                    available_samples = len(class_indices[class_label]) - class_counters[class_label]
                    actual_count = min(count, available_samples)
                    
                    start_idx = class_counters[class_label]
                    end_idx = start_idx + actual_count
                    
                    client_indices.extend(class_indices[class_label][start_idx:end_idx])
                    class_counters[class_label] = end_idx
            
            # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œä»å…¶ä»–ç±»åˆ«è¡¥å……
            while len(client_indices) < self.min_samples:
                for class_label in class_indices:
                    if class_counters[class_label] < len(class_indices[class_label]):
                        client_indices.append(class_indices[class_label][class_counters[class_label]])
                        class_counters[class_label] += 1
                        if len(client_indices) >= self.min_samples:
                            break
            
            self.client_indices[client_id] = client_indices
        
        # æ‰“å°åˆ†å¸ƒç»Ÿè®¡
        self._print_distribution_stats()
    
    def _print_distribution_stats(self):
        """æ‰“å°æ•°æ®åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯"""
        print(f"ğŸ“Š Non-IIDæ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
        
        # ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«åˆ†å¸ƒ
        client_class_counts = {}
        total_samples_per_client = []
        
        for client_id in range(min(10, self.num_clients)):  # åªæ˜¾ç¤ºå‰10ä¸ªå®¢æˆ·ç«¯
            class_count = {}
            indices = self.client_indices[client_id]
            total_samples_per_client.append(len(indices))
            
            for idx in indices:
                label = self.train_labels[idx]
                class_count[label] = class_count.get(label, 0) + 1
            
            client_class_counts[client_id] = class_count
            
            # æ‰“å°å‰5ä¸ªå®¢æˆ·ç«¯çš„è¯¦ç»†åˆ†å¸ƒ
            if client_id < 5:
                class_dist = [class_count.get(i, 0) for i in range(10)]
                print(f"   å®¢æˆ·ç«¯{client_id:2d}: {class_dist} (æ€»è®¡: {len(indices)})")
        
        # è®¡ç®—Non-IIDç¨‹åº¦
        avg_samples = np.mean(total_samples_per_client)
        std_samples = np.std(total_samples_per_client)
        
        print(f"   å¹³å‡æ ·æœ¬æ•°: {avg_samples:.1f} Â± {std_samples:.1f}")
        
        # è®¡ç®—ç±»åˆ«åˆ†å¸ƒçš„ä¸å‡åŒ€æ€§
        all_class_distributions = []
        for client_id in range(self.num_clients):
            indices = self.client_indices[client_id]
            class_count = [0] * 10
            for idx in indices:
                label = self.train_labels[idx]
                class_count[label] += 1
            
            # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            total = sum(class_count)
            if total > 0:
                class_prob = [c / total for c in class_count]
                all_class_distributions.append(class_prob)
        
        # è®¡ç®—å¹³å‡KLæ•£åº¦ï¼ˆè¡¡é‡Non-IIDç¨‹åº¦ï¼‰
        uniform_dist = [0.1] * 10  # å‡åŒ€åˆ†å¸ƒ
        kl_divergences = []
        
        for dist in all_class_distributions:
            kl_div = sum(p * np.log(p / q + 1e-10) if p > 0 else 0 
                        for p, q in zip(dist, uniform_dist))
            kl_divergences.append(kl_div)
        
        avg_kl = np.mean(kl_divergences)
        print(f"   Non-IIDç¨‹åº¦ (å¹³å‡KLæ•£åº¦): {avg_kl:.4f} (è¶Šå¤§è¶ŠNon-IID)")
        
        if avg_kl < 0.1:
            print("   ğŸŸ¢ æ•°æ®åˆ†å¸ƒæ¥è¿‘IID")
        elif avg_kl < 0.5:
            print("   ğŸŸ¡ ä¸­ç­‰ç¨‹åº¦Non-IID")
        else:
            print("   ğŸ”´ é«˜åº¦Non-IID")
    
    def get_client_loader(self, client_id, batch_size=10, shuffle=True):
        """è·å–æŒ‡å®šå®¢æˆ·ç«¯çš„æ•°æ®åŠ è½½å™¨"""
        if client_id not in self.client_indices:
            raise ValueError(f"å®¢æˆ·ç«¯{client_id}ä¸å­˜åœ¨")
        
        indices = self.client_indices[client_id]
        subset = Subset(self.train_dataset, indices)
        
        return DataLoader(
            subset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0
        )
    
    def get_test_loader(self, batch_size=1000):
        """è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0
        )
    
    def get_client_data_info(self, client_id):
        """è·å–å®¢æˆ·ç«¯æ•°æ®ä¿¡æ¯"""
        if client_id not in self.client_indices:
            return None
        
        indices = self.client_indices[client_id]
        class_count = {}
        
        for idx in indices:
            label = self.train_labels[idx]
            class_count[label] = class_count.get(label, 0) + 1
        
        return {
            'total_samples': len(indices),
            'class_distribution': class_count,
            'dominant_classes': sorted(class_count.keys(), key=lambda k: class_count[k], reverse=True)[:3]
        }


def create_noiid_loaders(num_clients=100, alpha=0.5, batch_size=10, data_dir='./data', min_samples=50):
    """
    åˆ›å»ºNon-IIDæ•°æ®åŠ è½½å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        num_clients: å®¢æˆ·ç«¯æ•°é‡
        alpha: Dirichletåˆ†å¸ƒå‚æ•°ï¼Œè¶Šå°è¶ŠNon-IID
        batch_size: æ‰¹å¤§å°
        data_dir: æ•°æ®ç›®å½•
        min_samples: æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ€å°æ ·æœ¬æ•°
    
    Returns:
        tuple: (client_loaders, test_loader, data_manager)
    """
    data_manager = NonIIDMNISTData(
        data_dir=data_dir,
        num_clients=num_clients,
        alpha=alpha,
        min_samples=min_samples
    )
    
    # åˆ›å»ºæ‰€æœ‰å®¢æˆ·ç«¯çš„æ•°æ®åŠ è½½å™¨
    client_loaders = {}
    for client_id in range(num_clients):
        client_loaders[client_id] = data_manager.get_client_loader(
            client_id, 
            batch_size=batch_size
        )
    
    test_loader = data_manager.get_test_loader()
    
    return client_loaders, test_loader, data_manager
